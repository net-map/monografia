

@article{coursera,
title= {Stanford CS229 - Machine Learning - Ng},
journal= {},
author= {Andrew Ng},
year= {2008},
url= {},
license= {},
abstract= {#Course Description

This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs; VC theory; large margins); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.

#Prerequisites

Students are expected to have the following background:
Knowledge of basic computer science principles and skills, at a level sufficient to write a reasonably non-trivial computer program.
Familiarity with the basic probability theory. (CS109 or Stat116 is sufficient but not necessary.)
Familiarity with the basic linear algebra (any one of Math 51, Math 103, Math 113, or CS 205 would be much more than necessary.)

Introduction (1 class)

* Basic concepts. 

Supervised learning. (7 classes)

* Supervised learning setup. LMS.
* Logistic regression. Perceptron. Exponential family. 
* Generative learning algorithms. Gaussian discriminant analysis. Naive Bayes. 
* Support vector machines. 
* Model selection and feature selection. 
* Ensemble methods: Bagging, boosting. 
* Evaluating and debugging learning algorithms. 

Learning theory. (3 classes)

* Bias/variance tradeoff. Union and Chernoff/Hoeffding bounds. 
* VC dimension. Worst case (online) learning. 
* Practical advice on how to use learning algorithms. 

Unsupervised learning. (5 classes)

* Clustering. K-means.
* EM. Mixture of Gaussians.
* Factor analysis.
* PCA (Principal components analysis).
* ICA (Independent components analysis). 

Reinforcement learning and control. (4 classes)

* MDPs. Bellman equations. 
* Value iteration and policy iteration. 
* Linear quadratic regulation (LQR). LQG. 
* Q-learning. Value function approximation. 
* Policy search. Reinforce. POMDPs. 
},
keywords= {machine learning, statistics, Regression},
terms= {}
}

@book{statbook,
 author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
 title = {An Introduction to Statistical Learning: With Applications in R},
 year = {2014},
 isbn = {1461471370, 9781461471370},
 publisher = {Springer Publishing Company, Incorporated}
} 


@INPROCEEDINGS{uji, 
author={J. Torres-Sospedra and R. Montoliu and A. Martinez-Usó and J. P. Avariento and T. J. Arnau and M. Benedito-Bordonau and J. Huerta}, 
booktitle={Indoor Positioning and Indoor Navigation (IPIN), 2014 International Conference on}, 
title={UJIIndoorLoc: A new multi-building and multi-floor database for WLAN fingerprint-based indoor localization problems}, 
year={2014}, 
pages={261-270}, 
keywords={database management systems;mobile computing;wireless LAN;UJIIndoorLoc;WLAN fingerprint;WLAN fingerprinting;indoor localization problems;mobile computing;mobile sensing community;multibuilding database;multifloor database;multifloor localization database;research community;Buildings;Databases;Smart phones;Training;Wireless LAN;Wireless application protocol}, 
doi={10.1109/IPIN.2014.7275492}, 
month={Oct},
}


@Article{Nagi2013,
author="Nagi, Sajid
and Bhattacharyya, Dhruba Kr.",
title="Classification of microarray cancer data using ensemble approach",
journal="Network Modeling Analysis in Health Informatics and Bioinformatics",
year="2013",
volume="2",
number="3",
pages="159--173",
abstract="An ensemble of classifiers is created by combining predictions of multiple component classifiers for improving prediction performance. In this paper, we conduct experimental comparison of J48, NB, IBK on nine microarray cancer datasets and also analyze their performance with Bagging, Boosting and Stack Generalization. The experimental results show that all ensemble methods outperform the individual classification methods. We then present a method, referred to as SD-EnClass, for combining classifiers from different classification families into an ensemble, based on a simple estimation of each classifier's class performance. The experimental results show that the proposed model improves classification accuracy, in comparison to simply selecting the best classifier in the combination. In the second stage, we combine the results of our proposed method with the results of Boosting, Bagging and Stacking using the combining method proposed, to obtain results which are significantly better than using Boosting, Bagging or Stacking alone.",
issn="2192-6670",
doi="10.1007/s13721-013-0034-x",
url="http://dx.doi.org/10.1007/s13721-013-0034-x"
}



@INPROCEEDINGS{comparative, 
author={S. Bozkurt and G. Elibol and S. Gunal and U. Yayan}, 
booktitle={Innovations in Intelligent SysTems and Applications (INISTA), 2015 International Symposium on}, 
title={A comparative study on machine learning algorithms for indoor positioning}, 
year={2015}, 
pages={1-8}, 
keywords={RSSI;decision trees;indoor navigation;learning (artificial intelligence);pattern classification;AdaBoost ensemble algorithms;RSS values;UJIIndoorLoc indoor positioning database;bagging ensemble algorithms;decision tree classifier;fingerprinting based positioning;indoor positioning systems;k-NN algorithm;k-nearest neighbor algorithm;machine learning algorithms;position estimation;radio map;received signal strength values;Accuracy;Classification algorithms;Decision trees;Floors;Machine learning algorithms;Training;AdaBoost;Bagging;Bayes Net;Localization;Naïve Bayes;RF Map;Received Signal Strength (RSS);SMO;WEKA;classification;decision tree (J48);indoor positioning;machine learning algorithms;nearest neighbor (NN)}, 
doi={10.1109/INISTA.2015.7276725}, 
month={Sept},}

 @book{quinlan,
    address = {San Mateo, CA},
    author = {Ross Quinlan},
    publisher = {Morgan Kaufmann Publishers},
    title = {C4.5: Programs for Machine Learning},
    year = {1993}
 }
 
 @inproceedings{Freund,
    address = {San Francisco},
    author = {Yoav Freund and Robert E. Schapire},
    booktitle = {Thirteenth International Conference on Machine Learning},
    pages = {148-156},
    publisher = {Morgan Kaufmann},
    title = {Experiments with a new boosting algorithm},
    year = {1996}
 }
 
 
 @article{comparativeEN,
  author    = {Richard Maclin and
               David W. Opitz},
  title     = {Popular Ensemble Methods: An Empirical Study},
  journal   = {CoRR},
  volume    = {abs/1106.0257},
  year      = {2011},
  url       = {http://arxiv.org/abs/1106.0257},
  timestamp = {Mon, 05 Dec 2011 18:05:33 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1106-0257},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
 
 
 
 @MISC{explainingadaboost,
    author = {Robert E. Schapire},
    title = {Explaining AdaBoost},
    year = {2013}
}

@MISC{adaboost,
    author = {Yoav Freund and Robert E. Schapire},
    title = {    A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting },
    year = {1996}
}